{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\n",
    "from keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILES = [\n",
    "    '../input/fasttextcrawl300d2m/crawl-300d-2M.vec',\n",
    "    '../input/love840b300dtxt/glove.840B.300d.txt'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_MODELS = 2\n",
    "BATCH_SIZE = 512\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "EPOCHS = 4\n",
    "MAX_LEN = 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n",
    "\n",
    "\n",
    "def build_matrix(word_index, path):\n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return embedding_matrix\n",
    "    \n",
    "\n",
    "def build_model(embedding_matrix, num_aux_targets):\n",
    "    words = Input(shape=(MAX_LEN,))\n",
    "    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "\n",
    "    hidden = concatenate([\n",
    "        GlobalMaxPooling1D()(x),\n",
    "        GlobalAveragePooling1D()(x),\n",
    "    ])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    result = Dense(1, activation='sigmoid')(hidden)\n",
    "    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n",
    "    \n",
    "    model = Model(inputs=words, outputs=[result, aux_result])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model\n",
    "    \n",
    "\n",
    "def preprocess(data):\n",
    "    '''\n",
    "    Credit goes to https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n",
    "    '''\n",
    "    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "    def clean_special_chars(text, punct):\n",
    "        for p in punct:\n",
    "            text = text.replace(p, ' ')\n",
    "        return text\n",
    "\n",
    "    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
    "test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = preprocess(train['comment_text'])\n",
    "y_train = np.where(train['target'] >= 0.5, 1, 0)\n",
    "y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\n",
    "x_test = preprocess(test['comment_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(list(x_train) + list(x_test))\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)\n",
    "\n",
    "embedding_matrix = np.concatenate([build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/2\n",
      " - 780s - loss: 0.2382 - dense_3_loss: 0.1307 - dense_4_loss: 0.1075\n",
      "Epoch 2/2\n",
      " - 778s - loss: 0.2192 - dense_3_loss: 0.1162 - dense_4_loss: 0.1030\n",
      "Epoch 1/2\n",
      " - 777s - loss: 0.2115 - dense_3_loss: 0.1097 - dense_4_loss: 0.1018\n",
      "Epoch 2/2\n",
      " - 781s - loss: 0.2077 - dense_3_loss: 0.1063 - dense_4_loss: 0.1014\n",
      "Epoch 1/2\n",
      " - 777s - loss: 0.2024 - dense_3_loss: 0.1015 - dense_4_loss: 0.1009\n",
      "Epoch 2/2\n",
      " - 782s - loss: 0.1995 - dense_3_loss: 0.0989 - dense_4_loss: 0.1007\n",
      "Epoch 1/2\n",
      " - 776s - loss: 0.1953 - dense_3_loss: 0.0949 - dense_4_loss: 0.1004\n",
      "Epoch 2/2\n",
      " - 781s - loss: 0.1930 - dense_3_loss: 0.0927 - dense_4_loss: 0.1003\n",
      "Epoch 1/2\n",
      " - 782s - loss: 0.2378 - dense_7_loss: 0.1305 - dense_8_loss: 0.1073\n",
      "Epoch 2/2\n",
      " - 781s - loss: 0.2187 - dense_7_loss: 0.1158 - dense_8_loss: 0.1029\n",
      "Epoch 1/2\n",
      " - 782s - loss: 0.2109 - dense_7_loss: 0.1092 - dense_8_loss: 0.1017\n",
      "Epoch 2/2\n",
      " - 775s - loss: 0.2073 - dense_7_loss: 0.1059 - dense_8_loss: 0.1013\n",
      "Epoch 1/2\n",
      " - 784s - loss: 0.2021 - dense_7_loss: 0.1013 - dense_8_loss: 0.1008\n",
      "Epoch 2/2\n",
      " - 779s - loss: 0.1991 - dense_7_loss: 0.0985 - dense_8_loss: 0.1006\n",
      "Epoch 1/2\n",
      " - 786s - loss: 0.1950 - dense_7_loss: 0.0947 - dense_8_loss: 0.1003\n",
      "Epoch 2/2\n",
      " - 780s - loss: 0.1925 - dense_7_loss: 0.0923 - dense_8_loss: 0.1002\n"
     ]
    }
   ],
   "source": [
    "checkpoint_predictions = []\n",
    "weights = []\n",
    "\n",
    "for model_idx in range(NUM_MODELS):\n",
    "    model = build_model(embedding_matrix, y_aux_train.shape[-1])\n",
    "    for global_epoch in range(EPOCHS):\n",
    "        model.fit(\n",
    "            x_train,\n",
    "            [y_train, y_aux_train],\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=2,\n",
    "            verbose=2,\n",
    "            callbacks=[\n",
    "                LearningRateScheduler(lambda epoch: 1e-3 * (0.6 ** global_epoch))\n",
    "            ]\n",
    "        )\n",
    "        checkpoint_predictions.append(model.predict(x_test, batch_size=2048)[0].flatten())\n",
    "        weights.append(2 ** global_epoch)\n",
    "\n",
    "predictions = np.average(checkpoint_predictions, weights=weights, axis=0)\n",
    "\n",
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test['id'],\n",
    "    'prediction': predictions\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
